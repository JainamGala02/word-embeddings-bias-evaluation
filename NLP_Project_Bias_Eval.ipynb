{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nzZDUfKc0FJN",
        "cs4MoYcG-YGN",
        "uQnBxX_t-ePU",
        "NXqz32k3WRAZ",
        "ULAtiYQgGd64"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**\n",
        "This block imports the necessary libraries and modules required for various tasks in this notebook."
      ],
      "metadata": {
        "id": "nzZDUfKc0FJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.formula.api as sm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xj2AbQt0EPY",
        "outputId": "352a19af-c4ab-4149-9aa5-88ec755c1d7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL CLASS**\n",
        "The `LexiconSentimentAnalyzer` class is designed to build a sentiment analysis model using word embeddings and sentiment lexicons. It provides methods to load word embeddings, positive and negative lexicons, and generate feature vectors for text. The class trains a logistic regression model using lexicon words as labeled data and evaluates its performance. Additionally, it includes functionality to predict sentiment scores for new text and analyze bias by comparing sentiment scores across sentence pairs. This modular approach allows for efficient sentiment analysis and bias detection.\n"
      ],
      "metadata": {
        "id": "j6pdFM-S-UBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LexiconSentimentAnalyzer:\n",
        "    def __init__(self, embedding_dim=300):\n",
        "        \"\"\"\n",
        "        Initialize the LexiconSentimentAnalyzer with a specified embedding dimension.\n",
        "\n",
        "        :param embedding_dim: The dimension of the word embeddings (default is 300).\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.model = None\n",
        "        self.embeddings_dict = None\n",
        "        self.positive_lexicon = None\n",
        "        self.negative_lexicon = None\n",
        "\n",
        "    def load_embeddings(self, embeddings_path):\n",
        "        \"\"\"\n",
        "        Load word embeddings from the provided file path.\n",
        "\n",
        "        :param embeddings_path: The path to the word embeddings file.\n",
        "        \"\"\"\n",
        "        self.embeddings_dict = self.load_embeddings_dict(embeddings_path)\n",
        "\n",
        "    def load_embeddings_dict(self, filename: str) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load word embeddings from a file and store them in a dictionary.\n",
        "\n",
        "        :param filename: The file containing word embeddings.\n",
        "        :return: A dictionary mapping words to their corresponding embedding vectors.\n",
        "        \"\"\"\n",
        "        print(f\"Loading embeddings from {filename}...\")\n",
        "        embeddings_dict = {}\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            for line in tqdm(f, desc=\"Loading embeddings\"):\n",
        "                values = line.strip().split(' ')\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_dict[word] = vector\n",
        "        return embeddings_dict\n",
        "\n",
        "    def load_lexicons(self, pos_path, neg_path):\n",
        "        \"\"\"\n",
        "        Load positive and negative sentiment lexicons from the provided file paths.\n",
        "\n",
        "        :param pos_path: Path to the positive lexicon file.\n",
        "        :param neg_path: Path to the negative lexicon file.\n",
        "        \"\"\"\n",
        "        print(\"\\nLoading lexicons...\")\n",
        "        self.positive_lexicon = self.load_lexicon(pos_path)\n",
        "        self.negative_lexicon = self.load_lexicon(neg_path)\n",
        "        print(f\"Loaded {len(self.positive_lexicon)} positive and {len(self.negative_lexicon)} negative words\")\n",
        "\n",
        "    def load_lexicon(self, filename: str) -> set:\n",
        "        \"\"\"\n",
        "        Load a sentiment lexicon from a file. Each line in the file should represent a word.\n",
        "\n",
        "        :param filename: Path to the lexicon file.\n",
        "        :return: A set of words in the lexicon.\n",
        "        \"\"\"\n",
        "        lexicon = set()\n",
        "        with open(filename, encoding='latin-1') as infile:\n",
        "            for line in infile:\n",
        "                line = line.rstrip()\n",
        "                if line and not line.startswith(';'):\n",
        "                    lexicon.add(line.lower())\n",
        "        return lexicon\n",
        "\n",
        "    def create_features(self, text: str, embeddings_dict: Dict[str, np.ndarray], embedding_dim: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create a feature vector for a given text using word embeddings.\n",
        "\n",
        "        :param text: The input text to be converted into a feature vector.\n",
        "        :param embeddings_dict: The dictionary of word embeddings.\n",
        "        :param embedding_dim: The dimension of the word embeddings.\n",
        "        :return: A numpy array representing the average word embedding of the text.\n",
        "        \"\"\"\n",
        "        words = word_tokenize(str(text).lower())\n",
        "        vectors = []\n",
        "        for word in words:\n",
        "            if word in embeddings_dict:\n",
        "                vectors.append(embeddings_dict[word])\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        return np.zeros(embedding_dim)\n",
        "\n",
        "    def train(self, embeddings_path, pos_lexicon_path, neg_lexicon_path):\n",
        "        \"\"\"\n",
        "        Train the sentiment analysis model using the positive and negative lexicons.\n",
        "\n",
        "        :param embeddings_path: Path to the word embeddings file.\n",
        "        :param pos_lexicon_path: Path to the positive sentiment lexicon file.\n",
        "        :param neg_lexicon_path: Path to the negative sentiment lexicon file.\n",
        "        :return: The trained LexiconSentimentAnalyzer object.\n",
        "        \"\"\"\n",
        "        if self.embeddings_dict is None:\n",
        "            self.load_embeddings(embeddings_path)\n",
        "\n",
        "        if self.positive_lexicon is None or self.negative_lexicon is None:\n",
        "            self.load_lexicons(pos_lexicon_path, neg_lexicon_path)\n",
        "\n",
        "        print(\"\\nCreating training features from lexicons...\")\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for word in tqdm(self.positive_lexicon, desc=\"Processing positive words\"):\n",
        "            if word in self.embeddings_dict:\n",
        "                features = self.create_features(word, self.embeddings_dict, self.embedding_dim)\n",
        "                X.append(features)\n",
        "                y.append(1)\n",
        "\n",
        "        for word in tqdm(self.negative_lexicon, desc=\"Processing negative words\"):\n",
        "            if word in self.embeddings_dict:\n",
        "                features = self.create_features(word, self.embeddings_dict, self.embedding_dim)\n",
        "                X.append(features)\n",
        "                y.append(0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(\"\\nTraining model...\")\n",
        "        self.model = LogisticRegression(\n",
        "            random_state=42,\n",
        "            max_iter=1000,\n",
        "            C=0.1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        train_score = self.model.score(X_train, y_train)\n",
        "        test_score = self.model.score(X_test, y_test)\n",
        "        print(f\"\\nModel Performance:\")\n",
        "        print(f\"Training accuracy: {train_score:.4f}\")\n",
        "        print(f\"Testing accuracy: {test_score:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, self.model.predict(X_test)))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_sentiment(self, text):\n",
        "        \"\"\"\n",
        "        Predict sentiment for a given text.\n",
        "\n",
        "        :param text: The input text for sentiment prediction.\n",
        "        :return: A dictionary containing sentiment score and probabilities of each class.\n",
        "        \"\"\"\n",
        "        if not self.model or not self.embeddings_dict:\n",
        "            raise ValueError(\"Model not trained or embeddings not loaded\")\n",
        "\n",
        "        features = self.create_features(\n",
        "            text,\n",
        "            self.embeddings_dict,\n",
        "            self.embedding_dim\n",
        "        )\n",
        "\n",
        "        probs = self.model.predict_proba([features])[0]\n",
        "        log_probs = np.log(probs)\n",
        "        sentiment_score = log_probs[1] - log_probs[0]\n",
        "\n",
        "        return {\n",
        "            'sentiment_score': sentiment_score,\n",
        "            'negative_prob': probs[0],\n",
        "            'positive_prob': probs[1]\n",
        "        }\n",
        "\n",
        "    def analyze_bias(self, sentence_pairs):\n",
        "        \"\"\"\n",
        "        Analyze potential bias in sentiment predictions for pairs of sentences.\n",
        "\n",
        "        :param sentence_pairs: A list of tuples containing pairs of sentences to compare.\n",
        "        :return: A list of dictionaries containing bias metrics for each sentence pair.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for sent1, sent2 in sentence_pairs:\n",
        "            score1 = self.predict_sentiment(sent1)\n",
        "            score2 = self.predict_sentiment(sent2)\n",
        "\n",
        "            bias_metrics = {\n",
        "                'sentence1': sent1,\n",
        "                'sentence2': sent2,\n",
        "                'sentiment_diff': score1['sentiment_score'] - score2['sentiment_score'],\n",
        "                'pos_prob_diff': score1['positive_prob'] - score2['positive_prob'],\n",
        "                'neg_prob_diff': score1['negative_prob'] - score2['negative_prob'],\n",
        "                'scores1': score1,\n",
        "                'scores2': score2\n",
        "            }\n",
        "            results.append(bias_metrics)\n",
        "        return results"
      ],
      "metadata": {
        "id": "NhOm-E_tMqAg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALYZE ON MULTIPLE EMBEDDINGS**\n",
        "The `MultiEmbeddingAnalyzer` class is designed to facilitate the analysis of biases across multiple word embeddings. It supports loading multiple embedding configurations, cleaning and preprocessing text, and training sentiment analyzers for each embedding. The class also provides methods for bias analysis, including comparisons across embeddings, visualizations of bias heatmaps, and statistical evaluations."
      ],
      "metadata": {
        "id": "cs4MoYcG-YGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiEmbeddingAnalyzer:\n",
        "    def __init__(self, embedding_configs: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Initialize analyzer with multiple embedding configurations.\n",
        "\n",
        "        :param embedding_configs: A dictionary of embedding configurations where each key is the\n",
        "                                  name of the embedding model (e.g., 'GloVe', 'GN-GloVe') and the\n",
        "                                  value contains a dictionary with 'path' to the model file\n",
        "                                  and 'dim' representing the dimensionality of the embeddings.\n",
        "        :type embedding_configs: Dict[str, Dict]\n",
        "        \"\"\"\n",
        "        self.embedding_configs = embedding_configs\n",
        "        self.analyzers = {}\n",
        "        self.results_cache = {}\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and preprocess the input text.\n",
        "\n",
        "        :param text: The input text to be cleaned.\n",
        "        :return: The cleaned text, which is in lowercase and stripped of non-alphabetic characters.\n",
        "        \"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def load_embeddings(self, filename: str) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load word embeddings from a file.\n",
        "\n",
        "        :param filename: The path to the file containing the word embeddings.\n",
        "        :return: A dictionary where each key is a word and the value is its corresponding embedding vector.\n",
        "        \"\"\"\n",
        "        print(f\"Loading embeddings from {filename}...\")\n",
        "        embeddings_dict = {}\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            for line in tqdm(f, desc=\"Loading embeddings\"):\n",
        "                values = line.strip().split(' ')\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_dict[word] = vector\n",
        "        return embeddings_dict\n",
        "\n",
        "    def load_lexicon(self, filename: str) -> set:\n",
        "        \"\"\"\n",
        "        Load a sentiment lexicon.\n",
        "\n",
        "        :param filename: The path to the sentiment lexicon file.\n",
        "        :return: A set containing all the words in the lexicon.\n",
        "        \"\"\"\n",
        "        lexicon = set()\n",
        "        with open(filename, encoding='latin-1') as infile:\n",
        "            for line in infile:\n",
        "                line = line.rstrip()\n",
        "                if line and not line.startswith(';'):\n",
        "                    lexicon.add(line.lower())\n",
        "        return lexicon\n",
        "\n",
        "    def train_all(self, pos_lexicon_path: str, neg_lexicon_path: str):\n",
        "        \"\"\"\n",
        "        Train analyzers for all configured embeddings.\n",
        "\n",
        "        :param pos_lexicon_path: Path to the positive sentiment lexicon file.\n",
        "        :param neg_lexicon_path: Path to the negative sentiment lexicon file.\n",
        "        \"\"\"\n",
        "        for embedding_name, config in self.embedding_configs.items():\n",
        "            print(f\"\\nTraining analyzer for {embedding_name}...\")\n",
        "            analyzer = LexiconSentimentAnalyzer(embedding_dim=config['dim'])\n",
        "            analyzer.train(\n",
        "                embeddings_path=config['path'],\n",
        "                pos_lexicon_path=pos_lexicon_path,\n",
        "                neg_lexicon_path=neg_lexicon_path\n",
        "            )\n",
        "            self.analyzers[embedding_name] = analyzer\n",
        "\n",
        "    def analyze_bias_all(self, test_pairs: Dict[str, List[Tuple[str, str]]], names_dict: Dict[str, List[str]]):\n",
        "        \"\"\"\n",
        "        Analyze bias across all embeddings.\n",
        "\n",
        "        :param test_pairs: A dictionary with categories as keys and their respective test pairs as values.\n",
        "                            Example: {'gender': [('He is ambitious', 'She is ambitious'), ...]}\n",
        "        :param names_dict: A dictionary of names grouped by category.\n",
        "        :return: A dictionary containing the results of the bias analysis for each embedding.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for embedding_name, analyzer in self.analyzers.items():\n",
        "            print(f\"\\nAnalyzing bias for {embedding_name}...\")\n",
        "\n",
        "            category_results = {}\n",
        "            for category, pairs in test_pairs.items():\n",
        "                category_results[category] = analyzer.analyze_bias(pairs)\n",
        "\n",
        "            name_sentiments = self._analyze_name_sentiments(analyzer, names_dict)\n",
        "\n",
        "            results[embedding_name] = {\n",
        "                'category_results': category_results,\n",
        "                'name_sentiments': name_sentiments\n",
        "            }\n",
        "\n",
        "        self.results_cache = results\n",
        "        return results\n",
        "\n",
        "    def _analyze_name_sentiments(self, analyzer, names_dict):\n",
        "        \"\"\"\n",
        "        Analyze sentiment for names and return a pandas DataFrame.\n",
        "\n",
        "        :param analyzer: The analyzer object to use for sentiment prediction.\n",
        "        :param names_dict: A dictionary of names grouped by category.\n",
        "        :return: A pandas DataFrame containing the sentiment analysis results for each name.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for group, names in names_dict.items():\n",
        "            for name in names:\n",
        "                sentiment = analyzer.predict_sentiment(name)\n",
        "                results.append({\n",
        "                    'name': name,\n",
        "                    'group': group,\n",
        "                    'sentiment': sentiment['sentiment_score']\n",
        "                })\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def plot_comparative_results(self, test_pairs: Dict[str, List[Tuple[str, str]]], save_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Generate comparative plots for all embeddings based on the bias analysis results.\n",
        "\n",
        "        :param test_pairs: A dictionary with categories and their test pairs.\n",
        "        :param save_dir: The directory where the plots will be saved. If None, the plots will be shown instead.\n",
        "        \"\"\"\n",
        "        if not self.results_cache:\n",
        "            raise ValueError(\"No results to plot. Run analyze_bias_all first.\")\n",
        "\n",
        "        if save_dir:\n",
        "            save_path = Path(save_dir)\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self._plot_category_heatmap(test_pairs, save_dir)\n",
        "        self._plot_embedding_comparison(test_pairs, save_dir)\n",
        "        self._plot_name_analysis(save_dir)\n",
        "\n",
        "    def _plot_category_heatmap(self, test_pairs: Dict[str, List[Tuple[str, str]]], save_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Create a heatmap showing bias across categories and embeddings.\n",
        "\n",
        "        :param test_pairs: A dictionary of test pairs categorized by type (e.g., 'gender', 'profession').\n",
        "        :param save_dir: The directory to save the heatmap plot.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        heat_data = []\n",
        "        for emb_name, results in self.results_cache.items():\n",
        "            for category, pairs in test_pairs.items():\n",
        "                for i, result in enumerate(results['category_results'][category]):\n",
        "                    pair_desc = f\"{category.title()} {i+1}\"\n",
        "                    heat_data.append({\n",
        "                        'Embedding': emb_name,\n",
        "                        'Comparison': pair_desc,\n",
        "                        'Bias Score': result['sentiment_diff']\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(heat_data)\n",
        "        pivot_table = df.pivot(index='Comparison', columns='Embedding', values='Bias Score')\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        sns.heatmap(pivot_table,\n",
        "                    cmap='RdBu_r',\n",
        "                    center=0,\n",
        "                    annot=True,\n",
        "                    fmt='.2f',\n",
        "                    cbar_kws={'label': 'Bias Score'},\n",
        "                    annot_kws={'size': 8})\n",
        "\n",
        "        plt.title('Bias Analysis Heatmap')\n",
        "\n",
        "        plt.tight_layout(pad=1.5)\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(Path(save_dir) / 'category_heatmap.png', bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def _plot_embedding_comparison(self, test_pairs: Dict[str, List[Tuple[str, str]]], save_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Compare the overall bias tendencies across different embeddings.\n",
        "\n",
        "        :param test_pairs: A dictionary of test pairs categorized by type (e.g., 'gender', 'profession').\n",
        "        :param save_dir: The directory to save the comparison plot.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        stats_data = []\n",
        "        for emb_name, results in self.results_cache.items():\n",
        "            for category in test_pairs.keys():\n",
        "                scores = [r['sentiment_diff'] for r in results['category_results'][category]]\n",
        "                stats_data.append({\n",
        "                    'Embedding': emb_name,\n",
        "                    'Category': category.title(),\n",
        "                    'Mean Bias': np.mean(scores),\n",
        "                    'Std Bias': np.std(scores),\n",
        "                    'Abs Mean Bias': np.mean(np.abs(scores))\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(stats_data)\n",
        "\n",
        "        sns.barplot(x='Embedding', y='Abs Mean Bias', hue='Category', data=df)\n",
        "\n",
        "        plt.title('Average Absolute Bias Magnitude by Embedding and Category\\n' +\n",
        "                 'Higher values indicate stronger biases regardless of direction')\n",
        "        plt.ylabel('Average Absolute Bias Magnitude')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title='Bias Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(Path(save_dir) / 'embedding_comparison.png')\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def _plot_name_analysis(self, save_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Plot sentiment analysis results for names.\n",
        "\n",
        "        :param save_dir: The directory to save the name analysis plot.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        name_stats = []\n",
        "        for emb_name, results in self.results_cache.items():\n",
        "            df = results['name_sentiments']\n",
        "            mean_sentiments = df.groupby('group')['sentiment'].mean().reset_index()\n",
        "            mean_sentiments['Embedding'] = emb_name\n",
        "            name_stats.append(mean_sentiments)\n",
        "\n",
        "        name_stats_df = pd.concat(name_stats)\n",
        "\n",
        "        sns.barplot(x='group', y='sentiment', hue='Embedding', data=name_stats_df)\n",
        "\n",
        "        plt.title('Average Name Sentiment by Demographic Group')\n",
        "        plt.xlabel('Demographic Group')\n",
        "        plt.ylabel('Average Sentiment Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title='Embedding Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(Path(save_dir) / 'name_analysis.png', bbox_inches='tight')\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def generate_statistical_report(self, save_dir: str = None):\n",
        "        \"\"\"\n",
        "        Generate a statistical report based on the bias analysis results.\n",
        "\n",
        "        :param save_dir: The directory to save the statistical report. If None, the report will not be saved.\n",
        "        \"\"\"\n",
        "        if not self.results_cache:\n",
        "            raise ValueError(\"No results to analyze. Run analyze_bias_all first.\")\n",
        "\n",
        "        report = {}\n",
        "\n",
        "        for embedding_name, results in self.results_cache.items():\n",
        "            name_sentiments = results['name_sentiments']\n",
        "            ols_model = sm.ols('sentiment ~ group', data=name_sentiments).fit()\n",
        "\n",
        "            summary_stats = name_sentiments.groupby('group')['sentiment'].describe()\n",
        "\n",
        "            groups = name_sentiments['group'].unique()\n",
        "            effect_sizes = {}\n",
        "            for i in range(len(groups)):\n",
        "                for j in range(i + 1, len(groups)):\n",
        "                    g1, g2 = groups[i], groups[j]\n",
        "                    g1_data = name_sentiments[name_sentiments['group'] == g1]['sentiment']\n",
        "                    g2_data = name_sentiments[name_sentiments['group'] == g2]['sentiment']\n",
        "                    pooled_std = np.sqrt((g1_data.var() + g2_data.var()) / 2)\n",
        "                    d = (g1_data.mean() - g2_data.mean()) / pooled_std\n",
        "                    effect_sizes[f\"{g1} vs {g2}\"] = float(d)\n",
        "\n",
        "            report[embedding_name] = {\n",
        "                'summary_statistics': summary_stats.to_dict(),\n",
        "                'ols_results': {\n",
        "                    'r_squared': float(ols_model.rsquared),\n",
        "                    'adj_r_squared': float(ols_model.rsquared_adj),\n",
        "                    'f_value': float(ols_model.fvalue),\n",
        "                    'p_value': float(ols_model.f_pvalue)\n",
        "                },\n",
        "                'effect_sizes': effect_sizes\n",
        "            }\n",
        "\n",
        "        if save_dir:\n",
        "            save_path = Path(save_dir)\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "            with open(save_path / 'statistical_report.json', 'w') as f:\n",
        "                json.dump(report, f, indent=2)\n",
        "\n",
        "        return report"
      ],
      "metadata": {
        "id": "LO6Y7iW8M3N1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGS**\n",
        "This block sets up and runs the bias analysis pipeline using the `MultiEmbeddingAnalyzer`. It defines configurations for multiple embeddings, including paths and dimensions, and specifies test sentence pairs and names categorized by ethnicity for bias evaluation. The embeddings are trained using positive and negative sentiment lexicons, and bias is analyzed across categories like profession, gender, and race. The results are visualized in plots and saved as statistical reports for further study. Finally, a summary of key findings, including R-squared values and effect sizes, is printed for quick reference."
      ],
      "metadata": {
        "id": "uQnBxX_t-ePU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_configs = {\n",
        "    'GloVe': {\n",
        "        'path': '/content/drive/MyDrive/My_Dataset/glove.6B.300d.txt',\n",
        "        'dim': 300\n",
        "    },\n",
        "    'GN-Glove': {\n",
        "        'path': '/content/drive/MyDrive/My_Dataset/gn-glove.txt',\n",
        "        'dim': 300\n",
        "    },\n",
        "    'Glove-hard-debiased': {\n",
        "        'path': '/content/drive/MyDrive/My_Dataset/glove_small_hard_debiased.txt',\n",
        "        'dim': 300\n",
        "    },\n",
        "    'Glove-INLP': {\n",
        "        'path': '/content/drive/MyDrive/My_Dataset/glove.42B.300d.projected.txt',\n",
        "        'dim': 300\n",
        "    },\n",
        "    'Conceptnet': {\n",
        "        'path': '/content/drive/MyDrive/My_Dataset/numberbatch-en.txt',\n",
        "        'dim': 300\n",
        "    }\n",
        "}\n",
        "\n",
        "test_pairs = {\n",
        "    'profession': [\n",
        "        (\"The engineer solved the problem\", \"The nurse solved the problem\"),\n",
        "        (\"The doctor made a diagnosis\", \"The nurse made a diagnosis\"),\n",
        "        (\"The CEO made a decision\", \"The secretary made a decision\"),\n",
        "        (\"The programmer wrote code\", \"The teacher taught class\"),\n",
        "        (\"The pilot flew the plane\", \"The flight attendant served drinks\")\n",
        "    ],\n",
        "    'gender': [\n",
        "        (\"He is ambitious\", \"She is ambitious\"),\n",
        "        (\"He is logical\", \"She is logical\"),\n",
        "        (\"He is strong\", \"She is strong\"),\n",
        "        (\"He is a leader\", \"She is a leader\"),\n",
        "        (\"His technical skills\", \"Her technical skills\")\n",
        "    ],\n",
        "    'race': [\n",
        "        (\"European neighborhood is safe\", \"African American neighborhood is safe\"),\n",
        "        (\"Asian students are hardworking\", \"Latino students are hardworking\"),\n",
        "        (\"White employees are professional\", \"Black employees are professional\"),\n",
        "        (\"European names sound familiar\", \"Middle Eastern names sound familiar\"),\n",
        "        (\"Western culture is advanced\", \"Eastern culture is advanced\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "NAMES_BY_ETHNICITY = {\n",
        "'White': [\n",
        "    'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
        "    'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
        "    'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
        "    'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
        "    'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
        "    'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
        "    'Megan', 'Rachel', 'Wendy'\n",
        "],\n",
        "'Black': [\n",
        "    'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
        "    'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
        "    'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
        "    'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
        "    'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
        "    'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
        "    'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
        "    'Tawanda', 'Yvette'\n",
        "],\n",
        "'Hispanic': [\n",
        "    'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
        "    'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
        "    'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
        "    'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
        "],\n",
        "'Arab/Muslim': [\n",
        "    'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
        "    'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
        "    'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
        "    'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
        "],\n",
        "'Hindu': [\n",
        "    'Aarav', 'Advait', 'Aryan', 'Dhruv', 'Eshan', 'Harsh', 'Ishaan', 'Kabir',\n",
        "    'Krishna', 'Laksh', 'Manan', 'Omkar', 'Raghav', 'Samar', 'Vedant', 'Aarya',\n",
        "    'Anaya', 'Charvi', 'Diya', 'Gauri', 'Ishita', 'Kavya', 'Meera', 'Nisha',\n",
        "    'Riya', 'Saisha', 'Sanya', 'Tanvi', 'Vaidehi', 'Yashasvi'\n",
        "]}\n",
        "\n",
        "analyzer = MultiEmbeddingAnalyzer(embedding_configs)\n",
        "\n",
        "analyzer.train_all(\n",
        "    pos_lexicon_path=\"/content/drive/MyDrive/My_Dataset/positive-words.txt\",\n",
        "    neg_lexicon_path=\"/content/drive/MyDrive/My_Dataset/negative-words.txt\"\n",
        ")\n",
        "\n",
        "results = analyzer.analyze_bias_all(test_pairs, NAMES_BY_ETHNICITY)\n",
        "\n",
        "analyzer.plot_comparative_results(test_pairs, save_dir=\"results/plots\")\n",
        "\n",
        "stats_report = analyzer.generate_statistical_report(save_dir=\"results/stats\")\n",
        "\n",
        "print(\"\\nSummary of Findings:\")\n",
        "for embedding, stats in stats_report.items():\n",
        "    print(f\"\\n{embedding}:\")\n",
        "    print(f\"R-squared: {stats['ols_results']['r_squared']:.3f}\")\n",
        "    print(f\"F-value: {stats['ols_results']['f_value']:.3f}\")\n",
        "    print(\"Effect sizes:\")\n",
        "    for comparison, effect_size in stats['effect_sizes'].items():\n",
        "        print(f\"  {comparison}: {effect_size:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZZCDmzXNAir",
        "outputId": "c0eb627f-2369-429e-fd07-d8a26b724e15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training analyzer for GloVe...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove.6B.300d.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 400000it [00:50, 7985.34it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 6917.78it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 7589.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9196\n",
            "Testing accuracy: 0.9255\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       843\n",
            "           1       0.87      0.91      0.89       405\n",
            "\n",
            "    accuracy                           0.93      1248\n",
            "   macro avg       0.91      0.92      0.92      1248\n",
            "weighted avg       0.93      0.93      0.93      1248\n",
            "\n",
            "\n",
            "Training analyzer for GN-Glove...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/gn-glove.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 142527it [00:15, 8936.75it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 8208.90it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 9335.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9195\n",
            "Testing accuracy: 0.8763\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91       777\n",
            "           1       0.79      0.85      0.82       379\n",
            "\n",
            "    accuracy                           0.88      1156\n",
            "   macro avg       0.86      0.87      0.86      1156\n",
            "weighted avg       0.88      0.88      0.88      1156\n",
            "\n",
            "\n",
            "Training analyzer for Glove-hard-debiased...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove_small_hard_debiased.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 42982it [00:07, 5526.20it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 10283.91it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 12896.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9501\n",
            "Testing accuracy: 0.9471\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96       513\n",
            "           1       0.96      0.90      0.93       319\n",
            "\n",
            "    accuracy                           0.95       832\n",
            "   macro avg       0.95      0.94      0.94       832\n",
            "weighted avg       0.95      0.95      0.95       832\n",
            "\n",
            "\n",
            "Training analyzer for Glove-INLP...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove.42B.300d.projected.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 1917495it [03:41, 8644.49it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 8274.77it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 8697.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9412\n",
            "Testing accuracy: 0.9231\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       910\n",
            "           1       0.85      0.92      0.88       416\n",
            "\n",
            "    accuracy                           0.92      1326\n",
            "   macro avg       0.91      0.92      0.91      1326\n",
            "weighted avg       0.93      0.92      0.92      1326\n",
            "\n",
            "\n",
            "Training analyzer for Conceptnet...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/numberbatch-en.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 516783it [00:52, 9788.40it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 6387.73it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6564.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9709\n",
            "Testing accuracy: 0.9667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       870\n",
            "           1       0.94      0.96      0.95       393\n",
            "\n",
            "    accuracy                           0.97      1263\n",
            "   macro avg       0.96      0.96      0.96      1263\n",
            "weighted avg       0.97      0.97      0.97      1263\n",
            "\n",
            "\n",
            "Analyzing bias for GloVe...\n",
            "\n",
            "Analyzing bias for GN-Glove...\n",
            "\n",
            "Analyzing bias for Glove-hard-debiased...\n",
            "\n",
            "Analyzing bias for Glove-INLP...\n",
            "\n",
            "Analyzing bias for Conceptnet...\n",
            "\n",
            "Summary of Findings:\n",
            "\n",
            "GloVe:\n",
            "R-squared: 0.189\n",
            "F-value: 10.775\n",
            "Effect sizes:\n",
            "  White vs Black: 0.897\n",
            "  White vs Hispanic: 0.409\n",
            "  White vs Arab/Muslim: 1.523\n",
            "  White vs Hindu: 0.695\n",
            "  Black vs Hispanic: -0.435\n",
            "  Black vs Arab/Muslim: 0.672\n",
            "  Black vs Hindu: -0.008\n",
            "  Hispanic vs Arab/Muslim: 1.055\n",
            "  Hispanic vs Hindu: 0.343\n",
            "  Arab/Muslim vs Hindu: -0.555\n",
            "\n",
            "GN-Glove:\n",
            "R-squared: 0.012\n",
            "F-value: 0.569\n",
            "Effect sizes:\n",
            "  White vs Black: -0.046\n",
            "  White vs Hispanic: 0.032\n",
            "  White vs Arab/Muslim: -0.086\n",
            "  White vs Hindu: 0.173\n",
            "  Black vs Hispanic: 0.134\n",
            "  Black vs Arab/Muslim: -0.083\n",
            "  Black vs Hindu: 0.228\n",
            "  Hispanic vs Arab/Muslim: -0.258\n",
            "  Hispanic vs Hindu: 0.175\n",
            "  Arab/Muslim vs Hindu: 0.258\n",
            "\n",
            "Glove-hard-debiased:\n",
            "R-squared: 0.443\n",
            "F-value: 36.785\n",
            "Effect sizes:\n",
            "  White vs Black: 2.021\n",
            "  White vs Hispanic: 0.791\n",
            "  White vs Arab/Muslim: 2.169\n",
            "  White vs Hindu: 2.330\n",
            "  Black vs Hispanic: -0.840\n",
            "  Black vs Arab/Muslim: -0.084\n",
            "  Black vs Hindu: 0.386\n",
            "  Hispanic vs Arab/Muslim: 0.838\n",
            "  Hispanic vs Hindu: 1.127\n",
            "  Arab/Muslim vs Hindu: 0.512\n",
            "\n",
            "Glove-INLP:\n",
            "R-squared: 0.090\n",
            "F-value: 4.550\n",
            "Effect sizes:\n",
            "  White vs Black: 0.218\n",
            "  White vs Hispanic: 1.059\n",
            "  White vs Arab/Muslim: 0.188\n",
            "  White vs Hindu: -0.255\n",
            "  Black vs Hispanic: 0.565\n",
            "  Black vs Arab/Muslim: -0.043\n",
            "  Black vs Hindu: -0.380\n",
            "  Hispanic vs Arab/Muslim: -0.669\n",
            "  Hispanic vs Hindu: -0.979\n",
            "  Arab/Muslim vs Hindu: -0.363\n",
            "\n",
            "Conceptnet:\n",
            "R-squared: 0.052\n",
            "F-value: 2.532\n",
            "Effect sizes:\n",
            "  White vs Black: 0.392\n",
            "  White vs Hispanic: 0.439\n",
            "  White vs Arab/Muslim: 0.722\n",
            "  White vs Hindu: 0.312\n",
            "  Black vs Hispanic: 0.060\n",
            "  Black vs Arab/Muslim: 0.368\n",
            "  Black vs Hindu: -0.018\n",
            "  Hispanic vs Arab/Muslim: 0.299\n",
            "  Hispanic vs Hindu: -0.066\n",
            "  Arab/Muslim vs Hindu: -0.316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bias Evaluation using the EEC (Equity Evaluation Corpus) Dataset**\n",
        "This block defines a comprehensive pipeline for analyzing bias in sentiment analysis models using the EEC dataset. The `BiasAnalyzer` class computes sentiment scores for sentences from the dataset, calculates bias metrics (such as gender and race biases), and generates plots to visualize these biases. It also includes functions for calculating effect sizes and performing statistical tests. The `BiasEvaluationPipeline` class facilitates the training of multiple sentiment analyzers, runs bias evaluation across various embeddings, and generates comparative plots to assess the effect of gender and race biases across different models. Results are saved as CSV files and visualizations for further analysis."
      ],
      "metadata": {
        "id": "NXqz32k3WRAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from tqdm import tqdm\n",
        "import scipy.stats as stats\n",
        "import json\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class BiasAnalyzer:\n",
        "    def __init__(self, sentiment_analyzer):\n",
        "        \"\"\"\n",
        "        Initialize bias analyzer with a trained sentiment analyzer\n",
        "\n",
        "        :param sentiment_analyzer: Trained sentiment analyzer with predict_sentiment method\n",
        "        \"\"\"\n",
        "        self.analyzer = sentiment_analyzer\n",
        "        self.results = {}\n",
        "\n",
        "        plt.style.use('default')\n",
        "        sns.set_theme()\n",
        "\n",
        "    def load_data(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"Load and preprocess the EEC dataset\n",
        "\n",
        "        :param filepath: Path to the CSV file containing the dataset\n",
        "        :return: A pandas DataFrame containing the preprocessed dataset\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(filepath)\n",
        "        required_cols = ['ID', 'Sentence', 'Gender', 'Race', 'Emotion']\n",
        "\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            raise ValueError(f\"Dataset must contain all required columns: {required_cols}\")\n",
        "\n",
        "        df['Gender'] = df['Gender'].str.strip().str.lower()\n",
        "        df['Race'] = df['Race'].str.strip().str.lower()\n",
        "        df['Emotion'] = df['Emotion'].str.strip().str.lower()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def compute_sentiment_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute sentiment scores for all sentences\n",
        "\n",
        "        :param df: A pandas DataFrame containing sentences to analyze\n",
        "        :return: A pandas DataFrame with sentiment scores for each sentence\n",
        "        \"\"\"\n",
        "        print(\"Computing sentiment scores for sentences...\")\n",
        "\n",
        "        scores = []\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            try:\n",
        "                score = self.analyzer.predict_sentiment(row['Sentence'])\n",
        "                sentiment_score = float(score['sentiment_score'])\n",
        "\n",
        "                scores.append({\n",
        "                    'ID': row['ID'],\n",
        "                    'sentence': row['Sentence'],\n",
        "                    'sentiment_score': sentiment_score,\n",
        "                    'gender': row['Gender'],\n",
        "                    'race': row['Race'],\n",
        "                    'emotion': row['Emotion']\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sentence {row['ID']}: {e}\")\n",
        "                continue\n",
        "\n",
        "        scores_df = pd.DataFrame(scores)\n",
        "        scores_df['sentiment_score'] = pd.to_numeric(scores_df['sentiment_score'], errors='coerce')\n",
        "        scores_df = scores_df.dropna(subset=['sentiment_score'])\n",
        "\n",
        "        return scores_df\n",
        "\n",
        "    def calculate_bias_metrics(self, scores_df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Calculate bias metrics including effect sizes and statistical tests\n",
        "\n",
        "        :param scores_df: A pandas DataFrame with sentiment scores and categorical data\n",
        "        :return: A dictionary with bias metrics (e.g., gender, race, and emotion bias)\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        gender_groups = scores_df.groupby('gender')['sentiment_score']\n",
        "        gender_stats = stats.f_oneway(*[group.values for name, group in gender_groups])\n",
        "        gender_means = gender_groups.mean()\n",
        "\n",
        "        gender_categories = scores_df['gender'].unique()\n",
        "        if len(gender_categories) == 2:\n",
        "            gender_effect = self._calculate_cohens_d(\n",
        "                scores_df[scores_df['gender'] == gender_categories[0]]['sentiment_score'],\n",
        "                scores_df[scores_df['gender'] == gender_categories[1]]['sentiment_score']\n",
        "            )\n",
        "        else:\n",
        "            gender_effect = self._calculate_f_effect_size(scores_df, 'gender')\n",
        "\n",
        "        metrics['gender'] = {\n",
        "            'f_statistic': float(gender_stats[0]),\n",
        "            'p_value': float(gender_stats[1]),\n",
        "            'effect_size': float(gender_effect),\n",
        "            'means': gender_means.to_dict()\n",
        "        }\n",
        "\n",
        "        race_groups = scores_df.groupby('race')['sentiment_score']\n",
        "        race_stats = stats.f_oneway(*[group.values for name, group in race_groups])\n",
        "        race_means = race_groups.mean()\n",
        "\n",
        "        race_effect = self._calculate_f_effect_size(scores_df, 'race')\n",
        "\n",
        "        metrics['race'] = {\n",
        "            'f_statistic': float(race_stats[0]),\n",
        "            'p_value': float(race_stats[1]),\n",
        "            'effect_size': float(race_effect),\n",
        "            'means': race_means.to_dict()\n",
        "        }\n",
        "\n",
        "        emotion_groups = scores_df.groupby('emotion')['sentiment_score']\n",
        "        emotion_stats = stats.f_oneway(*[group.values for name, group in emotion_groups])\n",
        "\n",
        "        metrics['emotion'] = {\n",
        "            'f_statistic': float(emotion_stats[0]),\n",
        "            'p_value': float(emotion_stats[1])\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_f_effect_size(self, df: pd.DataFrame, category: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate effect size using F-statistic method for any number of groups.\n",
        "\n",
        "        :param df: A pandas DataFrame with sentiment scores\n",
        "        :param category: The categorical variable (e.g., 'gender', 'race', etc.)\n",
        "        :return: The calculated effect size\n",
        "        \"\"\"\n",
        "        try:\n",
        "            groups = [group['sentiment_score'].values for _, group in df.groupby(category)]\n",
        "            f_stat = stats.f_oneway(*groups)[0]\n",
        "            n = len(df)\n",
        "            k = len(groups)\n",
        "            effect_size = (f_stat - (k-1)) / (f_stat + (n-k))\n",
        "            return max(0, effect_size)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating effect size for {category}: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_cohens_d(self, group1: pd.Series, group2: pd.Series) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Cohen's d effect size between two groups.\n",
        "\n",
        "        :param group1: First group of data\n",
        "        :param group2: Second group of data\n",
        "        :return: The calculated Cohen's d effect size\n",
        "        \"\"\"\n",
        "        try:\n",
        "            n1, n2 = len(group1), len(group2)\n",
        "            if n1 < 2 or n2 < 2:\n",
        "                return 0.0\n",
        "\n",
        "            var1, var2 = group1.var(), group2.var()\n",
        "\n",
        "            pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
        "\n",
        "            if pooled_se == 0:\n",
        "                return 0.0\n",
        "\n",
        "            return abs((group1.mean() - group2.mean()) / pooled_se)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating Cohen's d: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def plot_comparative_results(self, scores_df: pd.DataFrame, embedding_name: str,\n",
        "                               save_dir: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Generate comparative plots for bias analysis.\n",
        "\n",
        "        :param scores_df: A pandas DataFrame containing sentiment scores and categories\n",
        "        :param embedding_name: The name of the embedding used for sentiment analysis\n",
        "        :param save_dir: Directory to save the generated plots (optional)\n",
        "        \"\"\"\n",
        "        if save_dir:\n",
        "            save_path = Path(save_dir)\n",
        "            save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        sns.boxplot(data=scores_df, x='emotion', y='sentiment_score', hue='gender')\n",
        "        plt.title(f'{embedding_name}: Sentiment Scores by Gender and Emotion')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title='Gender')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(Path(save_dir) / 'gender_emotion_bias.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        sns.boxplot(data=scores_df, x='emotion', y='sentiment_score', hue='race')\n",
        "        plt.title(f'{embedding_name}: Sentiment Scores by Race and Emotion')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title='Race', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_dir:\n",
        "            plt.savefig(Path(save_dir) / 'race_emotion_bias.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def run_analysis(self, data_path: str, embedding_name: str,\n",
        "                    save_dir: Optional[str] = None) -> Tuple[pd.DataFrame, Dict]:\n",
        "        \"\"\"\n",
        "        Run the complete bias analysis pipeline.\n",
        "\n",
        "        :param data_path: Path to the dataset CSV file\n",
        "        :param embedding_name: The name of the embedding used for sentiment analysis\n",
        "        :param save_dir: Directory to save the generated results (optional)\n",
        "        :return: A tuple of pandas DataFrame with sentiment scores and bias metrics\n",
        "        \"\"\"\n",
        "        df = self.load_data(data_path)\n",
        "        print(f\"Loaded dataset with {len(df)} samples\")\n",
        "\n",
        "        scores_df = self.compute_sentiment_scores(df)\n",
        "        print(f\"Computed sentiment scores for {len(scores_df)} sentences\")\n",
        "\n",
        "        bias_metrics = self.calculate_bias_metrics(scores_df)\n",
        "\n",
        "        if save_dir:\n",
        "            print(f\"Generating plots in {save_dir}...\")\n",
        "            self.plot_comparative_results(scores_df, embedding_name, save_dir)\n",
        "\n",
        "            with open(Path(save_dir) / 'bias_metrics.json', 'w') as f:\n",
        "                json.dump(bias_metrics, f, indent=2)\n",
        "\n",
        "        return scores_df, bias_metrics\n",
        "\n",
        "class BiasEvaluationPipeline:\n",
        "    def __init__(self, embedding_configs: Dict):\n",
        "        \"\"\"\n",
        "        Initialize the bias evaluation pipeline with embedding configurations.\n",
        "\n",
        "        :param embedding_configs: A dictionary containing configurations for different embeddings\n",
        "        \"\"\"\n",
        "        self.embedding_configs = embedding_configs\n",
        "        self.output_dir = Path(\"bias_evaluation_results\")\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.analyzer = MultiEmbeddingAnalyzer(embedding_configs)\n",
        "        self.results = {}\n",
        "        self.bias_metrics = {}\n",
        "\n",
        "    def train_analyzers(self, pos_lexicon_path: str, neg_lexicon_path: str):\n",
        "        \"\"\"\n",
        "        Train sentiment analyzers for all embeddings.\n",
        "\n",
        "        :param pos_lexicon_path: Path to the positive lexicon file\n",
        "        :param neg_lexicon_path: Path to the negative lexicon file\n",
        "        \"\"\"\n",
        "        print(\"Training analyzers for all embeddings...\")\n",
        "        self.analyzer.train_all(\n",
        "            pos_lexicon_path=pos_lexicon_path,\n",
        "            neg_lexicon_path=neg_lexicon_path\n",
        "        )\n",
        "\n",
        "    def plot_comparative_analysis(self):\n",
        "        \"\"\"\n",
        "        Generate comparative plots across embeddings.\n",
        "\n",
        "        :return: A pandas DataFrame containing comparative metrics\n",
        "        \"\"\"\n",
        "        plot_data = []\n",
        "        for emb_name, metrics in self.bias_metrics.items():\n",
        "            plot_data.append({\n",
        "                'embedding': emb_name,\n",
        "                'gender_effect': metrics['gender']['effect_size'],\n",
        "                'race_effect': metrics['race']['effect_size']\n",
        "            })\n",
        "\n",
        "        plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "        plot_df = plot_df.fillna(0)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        x = np.arange(len(plot_df))\n",
        "        width = 0.35\n",
        "\n",
        "        plt.bar(x - width/2, plot_df['gender_effect'], width, label='Gender Bias')\n",
        "        plt.bar(x + width/2, plot_df['race_effect'], width, label='Race Bias')\n",
        "\n",
        "        plt.xlabel('Embeddings')\n",
        "        plt.ylabel('Effect Size')\n",
        "        plt.title('Comparative Bias Effect Sizes Across Embeddings')\n",
        "        plt.xticks(x, plot_df['embedding'], rotation=45)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(self.output_dir / 'comparative_bias.png', bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        plot_df.to_csv(self.output_dir / 'comparative_metrics.csv', index=False)\n",
        "\n",
        "        return plot_df\n",
        "\n",
        "    def run_evaluation(self, test_data_path: str):\n",
        "        \"\"\"\n",
        "        Run bias evaluation across all embeddings.\n",
        "\n",
        "        :param test_data_path: Path to the test dataset CSV file\n",
        "        :return: A tuple containing results, bias metrics, and comparative metrics\n",
        "        \"\"\"\n",
        "        if not self.analyzer.analyzers:\n",
        "            raise ValueError(\"Analyzers not trained. Call train_analyzers() first.\")\n",
        "\n",
        "        print(\"\\nRunning bias evaluation across embeddings...\")\n",
        "\n",
        "        for embedding_name, _ in self.embedding_configs.items():\n",
        "            print(f\"\\nAnalyzing bias for {embedding_name}...\")\n",
        "\n",
        "            sentiment_analyzer = self.analyzer.analyzers[embedding_name]\n",
        "            bias_analyzer = BiasAnalyzer(sentiment_analyzer)\n",
        "\n",
        "            scores_df, bias_metrics = bias_analyzer.run_analysis(\n",
        "                data_path=test_data_path,\n",
        "                embedding_name=embedding_name,\n",
        "                save_dir=self.output_dir / embedding_name\n",
        "            )\n",
        "\n",
        "            self.results[embedding_name] = scores_df\n",
        "            self.bias_metrics[embedding_name] = bias_metrics\n",
        "\n",
        "        comparative_metrics = self.plot_comparative_analysis()\n",
        "\n",
        "        print(f\"\\nResults and analysis saved in: {self.output_dir}\")\n",
        "\n",
        "        return self.results, self.bias_metrics, comparative_metrics"
      ],
      "metadata": {
        "id": "jIahEedC87Iq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main method to initialize the bias evaluation pipeline, train analyzers, and run the evaluation.\n",
        "\n",
        "    This function sets up the BiasEvaluationPipeline, trains sentiment analyzers using provided lexicons,\n",
        "    and runs the bias evaluation on a given dataset. It also prints the location of the saved results.\n",
        "\n",
        "    :return: A tuple containing the evaluation results, bias metrics, and comparative metrics\n",
        "    \"\"\"\n",
        "    pipeline = BiasEvaluationPipeline(embedding_configs)\n",
        "\n",
        "    pipeline.train_analyzers(\n",
        "        pos_lexicon_path=\"/content/drive/MyDrive/My_Dataset/positive-words.txt\",\n",
        "        neg_lexicon_path=\"/content/drive/MyDrive/My_Dataset/negative-words.txt\"\n",
        "    )\n",
        "\n",
        "    results, bias_metrics, comparative_metrics = pipeline.run_evaluation(\n",
        "        test_data_path=\"/content/drive/MyDrive/My_Dataset/EEC.csv\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\nResults and analysis saved in: {pipeline.output_dir}\")\n",
        "\n",
        "    return results, bias_metrics, comparative_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, bias_metrics, comparative_metrics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvV7n6QXtyE",
        "outputId": "47eb3031-7bbd-4143-d476-b4db9cc0f590"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training analyzers for all embeddings...\n",
            "\n",
            "Training analyzer for GloVe...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove.6B.300d.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 400000it [00:37, 10597.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 7983.28it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6086.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9196\n",
            "Testing accuracy: 0.9255\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       843\n",
            "           1       0.87      0.91      0.89       405\n",
            "\n",
            "    accuracy                           0.93      1248\n",
            "   macro avg       0.91      0.92      0.92      1248\n",
            "weighted avg       0.93      0.93      0.93      1248\n",
            "\n",
            "\n",
            "Training analyzer for GN-Glove...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/gn-glove.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 142527it [00:14, 10037.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 6788.73it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6760.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9195\n",
            "Testing accuracy: 0.8763\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91       777\n",
            "           1       0.79      0.85      0.82       379\n",
            "\n",
            "    accuracy                           0.88      1156\n",
            "   macro avg       0.86      0.87      0.86      1156\n",
            "weighted avg       0.88      0.88      0.88      1156\n",
            "\n",
            "\n",
            "Training analyzer for Glove-hard-debiased...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove_small_hard_debiased.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 42982it [00:05, 7177.53it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 11314.98it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 12072.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9501\n",
            "Testing accuracy: 0.9471\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96       513\n",
            "           1       0.96      0.90      0.93       319\n",
            "\n",
            "    accuracy                           0.95       832\n",
            "   macro avg       0.95      0.94      0.94       832\n",
            "weighted avg       0.95      0.95      0.95       832\n",
            "\n",
            "\n",
            "Training analyzer for Glove-INLP...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove.42B.300d.projected.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 1917495it [03:11, 10023.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 5390.62it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6229.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9412\n",
            "Testing accuracy: 0.9231\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       910\n",
            "           1       0.85      0.92      0.88       416\n",
            "\n",
            "    accuracy                           0.92      1326\n",
            "   macro avg       0.91      0.92      0.91      1326\n",
            "weighted avg       0.93      0.92      0.92      1326\n",
            "\n",
            "\n",
            "Training analyzer for Conceptnet...\n",
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/numberbatch-en.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 516783it [00:49, 10505.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 6048.94it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6005.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9709\n",
            "Testing accuracy: 0.9667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       870\n",
            "           1       0.94      0.96      0.95       393\n",
            "\n",
            "    accuracy                           0.97      1263\n",
            "   macro avg       0.96      0.96      0.96      1263\n",
            "weighted avg       0.97      0.97      0.97      1263\n",
            "\n",
            "\n",
            "Running bias evaluation across embeddings...\n",
            "\n",
            "Analyzing bias for GloVe...\n",
            "Loaded dataset with 8640 samples\n",
            "Computing sentiment scores for sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8640/8640 [00:05<00:00, 1587.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed sentiment scores for 8640 sentences\n",
            "Generating plots in bias_evaluation_results/GloVe...\n",
            "\n",
            "Analyzing bias for GN-Glove...\n",
            "Loaded dataset with 8640 samples\n",
            "Computing sentiment scores for sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8640/8640 [00:06<00:00, 1306.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed sentiment scores for 8640 sentences\n",
            "Generating plots in bias_evaluation_results/GN-Glove...\n",
            "\n",
            "Analyzing bias for Glove-hard-debiased...\n",
            "Loaded dataset with 8640 samples\n",
            "Computing sentiment scores for sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8640/8640 [00:05<00:00, 1605.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed sentiment scores for 8640 sentences\n",
            "Generating plots in bias_evaluation_results/Glove-hard-debiased...\n",
            "\n",
            "Analyzing bias for Glove-INLP...\n",
            "Loaded dataset with 8640 samples\n",
            "Computing sentiment scores for sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8640/8640 [00:06<00:00, 1384.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed sentiment scores for 8640 sentences\n",
            "Generating plots in bias_evaluation_results/Glove-INLP...\n",
            "\n",
            "Analyzing bias for Conceptnet...\n",
            "Loaded dataset with 8640 samples\n",
            "Computing sentiment scores for sentences...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8640/8640 [00:05<00:00, 1511.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed sentiment scores for 8640 sentences\n",
            "Generating plots in bias_evaluation_results/Conceptnet...\n",
            "\n",
            "Results and analysis saved in: bias_evaluation_results\n",
            "\n",
            "Results and analysis saved in: bias_evaluation_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST**\n",
        "This block is just used to test the sentiment scores produced by the model with a particular embedding for a custom sentence."
      ],
      "metadata": {
        "id": "ULAtiYQgGd64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = LexiconSentimentAnalyzer()\n",
        "\n",
        "analyzer.load_embeddings('/content/drive/MyDrive/My_Dataset/glove.6B.300d.txt')\n",
        "analyzer.load_lexicons('/content/drive/MyDrive/My_Dataset/positive-words.txt', '/content/drive/MyDrive/My_Dataset/negative-words.txt')\n",
        "\n",
        "analyzer.train('/content/glove.6B.300d.txt', '/content/drive/MyDrive/My_Dataset/positive-words.txt', '/content/drive/MyDrive/My_Dataset/negative-words.txt')\n",
        "\n",
        "sentence = \"He is ambitious\"\n",
        "sentence2 = \"She is ambitious\"\n",
        "sentiment_result = analyzer.predict_sentiment(sentence)\n",
        "sentiment_result2 = analyzer.predict_sentiment(sentence2)\n",
        "\n",
        "print(sentiment_result)\n",
        "print(sentiment_result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYgpdhKEFfYq",
        "outputId": "b490f057-e154-4347-8eaa-81c007089ec0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings from /content/drive/MyDrive/My_Dataset/glove.6B.300d.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading embeddings: 400000it [00:39, 10173.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading lexicons...\n",
            "Loaded 2006 positive and 4783 negative words\n",
            "\n",
            "Creating training features from lexicons...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing positive words: 100%|██████████| 2006/2006 [00:00<00:00, 5921.13it/s]\n",
            "Processing negative words: 100%|██████████| 4783/4783 [00:00<00:00, 6650.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Training accuracy: 0.9196\n",
            "Testing accuracy: 0.9255\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       843\n",
            "           1       0.87      0.91      0.89       405\n",
            "\n",
            "    accuracy                           0.93      1248\n",
            "   macro avg       0.91      0.92      0.92      1248\n",
            "weighted avg       0.93      0.93      0.93      1248\n",
            "\n",
            "{'sentiment_score': 2.606092748301978, 'negative_prob': 0.06874732930043348, 'positive_prob': 0.9312526706995665}\n",
            "{'sentiment_score': 3.094151493274395, 'negative_prob': 0.043349145785912646, 'positive_prob': 0.9566508542140874}\n"
          ]
        }
      ]
    }
  ]
}